{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gradio transformers torch librosa soundfile werkzeug\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsoWHBsEPRhr",
        "outputId": "89d1a2e2-a8d4-4161-9564-d4edcb7d0d65"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: werkzeug in /usr/local/lib/python3.10/dist-packages (3.0.6)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.4)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.1)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.2)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Acb4bp2VN6Mo"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "import os\n",
        "import gc\n",
        "import secrets\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CFG class serves as a centralized configuration holder for the TTS application. It defines various settings and parameters that dictate how the application behaves.\n",
        "\n",
        "VOICES: A dictionary that maps human-readable voice labels to their corresponding voice presets. For example:\n",
        "\n",
        "\"male EN\" corresponds to an English male voice.\n",
        "\"female Chinese\" corresponds to a Chinese female voice\n",
        "\n",
        "Post-processing Thresholds:\n",
        "\n",
        "AMPLITUDE_THRESHOLD: Sets the minimum amplitude level below which audio samples are considered noise and can be trimmed from the audio output.\n",
        "TIME_THRESHOLD: Defines the duration (in samples) to determine how long low-amplitude sections should be to qualify for trimming. Calculated as half a second at a 24kHz sample rate.\n",
        "IGNORE_INITIAL_STEPS: Specifies the number of initial samples to ignore when applying the amplitude threshold, allowing the model to skip any startup noise."
      ],
      "metadata": {
        "id": "nw-iaManf2As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration Class\n",
        "class CFG:\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Voice Presets\n",
        "    VOICES = {\n",
        "        \"male EN\": \"v2/en_speaker_6\",    # Example Male Voice 1 (English)\n",
        "        \"male Chinese\": \"v2/zh_speaker_3\",    # Example Male Voice 2 (Chinese)\n",
        "        \"female EN\": \"v2/en_speaker_9\",  # Example Female Voice 1 (English)\n",
        "        \"female Chinese\": \"v2/zh_speaker_4\",  # Example Female Voice 2 (Chinese)\n",
        "    }\n",
        "\n",
        "    # Model settings\n",
        "    MODEL_NAME = 'suno/bark'\n",
        "\n",
        "    # Post-processing thresholds\n",
        "    AMPLITUDE_THRESHOLD = 0.05\n",
        "    TIME_THRESHOLD = int(24_000 * 0.5)  # Half a second at 24kHz\n",
        "    IGNORE_INITIAL_STEPS = int(24_000 * 0.5)  # Ignore first half-second\n",
        "\n",
        "    # Paths\n",
        "    AUDIO_OUTPUT_PATH = '/content/static/audio'  # Directory to store generated audio files\n"
      ],
      "metadata": {
        "id": "ElK9JSiHN668"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(CFG.AUDIO_OUTPUT_PATH, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "uGwWHXTPN7BG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bark Model**\n",
        "\n",
        "The BARK model by Suno is a state-of-the-art Text-to-Speech (TTS) system designed to produce high-quality, natural-sounding speech. It supports multiple languages and voice presets, allowing users to select different voice characteristics such as gender and language. BARK leverages advanced deep learning techniques to understand and generate speech that closely mimics human intonation and rhythm."
      ],
      "metadata": {
        "id": "aYMhINxKf46O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Processor and Model once when the server starts\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    CFG.MODEL_NAME,\n",
        "    voice_preset=CFG.VOICES[\"male EN\"],  # Temporary default, will be overridden per request\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    CFG.MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(CFG.DEVICE)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irILZ6RpN7D_",
        "outputId": "02c14143-ee1b-4e3e-f69f-e94a2de36128"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BarkModel(\n",
              "  (semantic): BarkSemanticModel(\n",
              "    (input_embeds_layer): Embedding(129600, 1024)\n",
              "    (position_embeds_layer): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x BarkBlock(\n",
              "        (layernorm_1): BarkLayerNorm()\n",
              "        (layernorm_2): BarkLayerNorm()\n",
              "        (attn): BarkSelfAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (att_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        )\n",
              "        (mlp): BarkMLP(\n",
              "          (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm_final): BarkLayerNorm()\n",
              "    (lm_head): Linear(in_features=1024, out_features=10048, bias=False)\n",
              "  )\n",
              "  (coarse_acoustics): BarkCoarseModel(\n",
              "    (input_embeds_layer): Embedding(12096, 1024)\n",
              "    (position_embeds_layer): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x BarkBlock(\n",
              "        (layernorm_1): BarkLayerNorm()\n",
              "        (layernorm_2): BarkLayerNorm()\n",
              "        (attn): BarkSelfAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (att_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        )\n",
              "        (mlp): BarkMLP(\n",
              "          (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm_final): BarkLayerNorm()\n",
              "    (lm_head): Linear(in_features=1024, out_features=12096, bias=False)\n",
              "  )\n",
              "  (fine_acoustics): BarkFineModel(\n",
              "    (input_embeds_layers): ModuleList(\n",
              "      (0-7): 8 x Embedding(1056, 1024)\n",
              "    )\n",
              "    (position_embeds_layer): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x BarkBlock(\n",
              "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (layernorm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): BarkSelfAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (att_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        )\n",
              "        (mlp): BarkMLP(\n",
              "          (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (lm_heads): ModuleList(\n",
              "      (0-6): 7 x Linear(in_features=1024, out_features=1056, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (codec_model): EncodecModel(\n",
              "    (encoder): EncodecEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): EncodecConv1d(\n",
              "          (conv): Conv1d(1, 32, kernel_size=(7,), stride=(1,))\n",
              "        )\n",
              "        (1): EncodecResnetBlock(\n",
              "          (block): ModuleList(\n",
              "            (0): ELU(alpha=1.0)\n",
              "            (1): EncodecConv1d(\n",
              "              (conv): Conv1d(32, 16, kernel_size=(3,), stride=(1,))\n",
              "            )\n",
              "            (2): ELU(alpha=1.0)\n",
              "            (3): EncodecConv1d(\n",
              "              (conv): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "          (shortcut): EncodecConv1d(\n",
              "            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (2): ELU(alpha=1.0)\n",
              "        (3): EncodecConv1d(\n",
              "          (conv): Conv1d(32, 64, kernel_size=(4,), stride=(2,))\n",
              "        )\n",
              "        (4): EncodecResnetBlock(\n",
              "          (block): ModuleList(\n",
              "            (0): ELU(alpha=1.0)\n",
              "            (1): EncodecConv1d(\n",
              "              (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
              "            )\n",
              "            (2): ELU(alpha=1.0)\n",
              "            (3): EncodecConv1d(\n",
              "              (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "          (shortcut): EncodecConv1d(\n",
              "            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (5): ELU(alpha=1.0)\n",
              "        (6): EncodecConv1d(\n",
              "          (conv): Conv1d(64, 128, kernel_size=(8,), stride=(4,))\n",
              "        )\n",
              "        (7): EncodecResnetBlock(\n",
              "          (block): ModuleList(\n",
              "            (0): ELU(alpha=1.0)\n",
              "            (1): EncodecConv1d(\n",
              "              (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n",
              "            )\n",
              "            (2): ELU(alpha=1.0)\n",
              "            (3): EncodecConv1d(\n",
              "              (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "          (shortcut): EncodecConv1d(\n",
              "            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (8): ELU(alpha=1.0)\n",
              "        (9): EncodecConv1d(\n",
              "          (conv): Conv1d(128, 256, kernel_size=(10,), stride=(5,))\n",
              "        )\n",
              "        (10): EncodecResnetBlock(\n",
              "          (block): ModuleList(\n",
              "            (0): ELU(alpha=1.0)\n",
              "            (1): EncodecConv1d(\n",
              "              (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
              "            )\n",
              "            (2): ELU(alpha=1.0)\n",
              "            (3): EncodecConv1d(\n",
              "              (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "          (shortcut): EncodecConv1d(\n",
              "            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (11): ELU(alpha=1.0)\n",
              "        (12): EncodecConv1d(\n",
              "          (conv): Conv1d(256, 512, kernel_size=(16,), stride=(8,))\n",
              "        )\n",
              "        (13): EncodecLSTM(\n",
              "          (lstm): LSTM(512, 512, num_layers=2)\n",
              "        )\n",
              "        (14): ELU(alpha=1.0)\n",
              "        (15): EncodecConv1d(\n",
              "          (conv): Conv1d(512, 128, kernel_size=(7,), stride=(1,))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): EncodecDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): EncodecConv1d(\n",
              "          (conv): Conv1d(128, 512, kernel_size=(7,), stride=(1,))\n",
              "        )\n",
              "        (1): EncodecLSTM(\n",
              "          (lstm): LSTM(512, 512, num_layers=2)\n",
              "        )\n",
              "        (2): ELU(alpha=1.0)\n",
              "        (3): EncodecConvTranspose1d(\n",
              "          (conv): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,))\n",
              "        )\n",
              "        (4): EncodecResnetBlock(\n",
              "          (block): ModuleList(\n",
              "            (0): ELU(alpha=1.0)\n",
              "            (1): EncodecConv1d(\n",
              "              (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
              "            )\n",
              "            (2): ELU(alpha=1.0)\n",
              "            (3): EncodecConv1d(\n",
              "              (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "          (shortcut): EncodecConv1d(\n",
              "            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (5): ELU(alpha=1.0)\n",
              "        (6): EncodecConvTranspose1d(\n",
              "          (conv): ConvTranspose1d(256, 128, kernel_size=(10,), stride=(5,))\n",
              "        )\n",
              "        (7): EncodecResnetBlock(\n",
              "          (block): ModuleList(\n",
              "            (0): ELU(alpha=1.0)\n",
              "            (1): EncodecConv1d(\n",
              "              (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n",
              "            )\n",
              "            (2): ELU(alpha=1.0)\n",
              "            (3): EncodecConv1d(\n",
              "              (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "          (shortcut): EncodecConv1d(\n",
              "            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (8): ELU(alpha=1.0)\n",
              "        (9): EncodecConvTranspose1d(\n",
              "          (conv): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,))\n",
              "        )\n",
              "        (10): EncodecResnetBlock(\n",
              "          (block): ModuleList(\n",
              "            (0): ELU(alpha=1.0)\n",
              "            (1): EncodecConv1d(\n",
              "              (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
              "            )\n",
              "            (2): ELU(alpha=1.0)\n",
              "            (3): EncodecConv1d(\n",
              "              (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "          (shortcut): EncodecConv1d(\n",
              "            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (11): ELU(alpha=1.0)\n",
              "        (12): EncodecConvTranspose1d(\n",
              "          (conv): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,))\n",
              "        )\n",
              "        (13): EncodecResnetBlock(\n",
              "          (block): ModuleList(\n",
              "            (0): ELU(alpha=1.0)\n",
              "            (1): EncodecConv1d(\n",
              "              (conv): Conv1d(32, 16, kernel_size=(3,), stride=(1,))\n",
              "            )\n",
              "            (2): ELU(alpha=1.0)\n",
              "            (3): EncodecConv1d(\n",
              "              (conv): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "          (shortcut): EncodecConv1d(\n",
              "            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (14): ELU(alpha=1.0)\n",
              "        (15): EncodecConv1d(\n",
              "          (conv): Conv1d(32, 1, kernel_size=(7,), stride=(1,))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (quantizer): EncodecResidualVectorQuantizer(\n",
              "      (layers): ModuleList(\n",
              "        (0-31): 32 x EncodecVectorQuantization(\n",
              "          (codebook): EncodecEuclideanCodebook()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility Functions\n",
        "def split_sentences(text):\n",
        "    import re\n",
        "    sentences = re.split(r'\\. |\\.\\n|\\.\\n\\n|!|\\?|;', text)\n",
        "    sentences = [sentence.strip() + '.' for sentence in sentences if sentence.strip()]\n",
        "    number_of_sentences = len(sentences)\n",
        "    return sentences, number_of_sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "kK3mKz7VN7J4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(text, processor=processor):\n",
        "    return len(processor.tokenizer(text)['input_ids'])"
      ],
      "metadata": {
        "id": "WXWdzyvgdLEX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slice_array_wave(input_array, amplitude_threshold, time_threshold, ignore_initial_steps=0):\n",
        "    low_amplitude_indices = np.abs(input_array) < amplitude_threshold\n",
        "    consecutive_count = 0\n",
        "    for i, is_low_amplitude in enumerate(low_amplitude_indices[ignore_initial_steps:]):\n",
        "        if is_low_amplitude:\n",
        "            consecutive_count += 1\n",
        "        else:\n",
        "            consecutive_count = 0\n",
        "\n",
        "        if consecutive_count >= time_threshold:\n",
        "            return input_array[:i + int(time_threshold / 4)]\n",
        "\n",
        "    return input_array"
      ],
      "metadata": {
        "id": "bQgcd3obTpZb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perform_inference function is the heart of the TTS application. It orchestrates the conversion of input text into synthesized speech."
      ],
      "metadata": {
        "id": "NnUW7gx0f7k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_inference(text, voice_preset):\n",
        "    sentences, number_of_sentences = split_sentences(text)\n",
        "    print(f'\\nNumber of sentences in this text: {number_of_sentences}\\n')\n",
        "\n",
        "    all_audio_arrays = []\n",
        "\n",
        "    for sentence_number, current_sentence in enumerate(sentences, start=1):\n",
        "        print(f'Processing sentence {sentence_number}/{number_of_sentences}...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Prepare input\n",
        "        inputs = processor(\n",
        "            text=current_sentence,\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "            max_length=1024,\n",
        "            voice_preset=voice_preset,\n",
        "            add_special_tokens=False,\n",
        "        ).to(CFG.DEVICE)\n",
        "\n",
        "        # Count tokens\n",
        "        n_tokens = count_tokens(current_sentence, processor)\n",
        "\n",
        "        # Model inference\n",
        "        with torch.inference_mode():\n",
        "            result = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=True,\n",
        "                semantic_max_new_tokens=512,  # Reduced from 1024\n",
        "                pad_token_id=processor.tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        # Save results\n",
        "        audio_array = result.cpu().numpy().squeeze()\n",
        "        all_audio_arrays.append(audio_array)\n",
        "        elapsed_time = round((time.time() - start_time), 2)\n",
        "\n",
        "        print(f'''\n",
        "              Sentence {sentence_number}/{number_of_sentences} processed:\n",
        "              \tNumber of tokens in sentence: {n_tokens}\n",
        "              \tLength of sentence: {len(current_sentence)}\n",
        "              \tShape of tensor for this sentence: {result.size()}\n",
        "              \tElapsed time for this sentence: {elapsed_time} s\n",
        "              ''')\n",
        "\n",
        "        # Clean up\n",
        "        del result\n",
        "        gc.collect()\n",
        "\n",
        "    # Concatenate and post-process audio without slicing to ensure completeness\n",
        "    concatenated_array = np.concatenate(all_audio_arrays)\n",
        "\n",
        "    # Save as .wav file\n",
        "    filename = 'final_audio.wav'\n",
        "    filepath = os.path.join(CFG.AUDIO_OUTPUT_PATH, filename)\n",
        "    write(filepath, rate=24000, data=concatenated_array.astype(np.float32))  # Assuming 24,000 Hz sample rate\n",
        "    print(f\"Final audio saved as {filename}\")\n",
        "\n",
        "    return filepath\n"
      ],
      "metadata": {
        "id": "P5yP6wIbN7ND"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generate_tts function acts as the bridge between the user interface (Gradio) and the core TTS processing logic."
      ],
      "metadata": {
        "id": "gT65KjgAf86F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Inference Function for Gradio\n",
        "def generate_tts(text, voice):\n",
        "    audio_path = perform_inference(text, CFG.VOICES[voice])\n",
        "    return audio_path"
      ],
      "metadata": {
        "id": "qKukn4ZlN7QI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section sets up the user interface (UI) using Gradio, a Python library that simplifies the creation of web-based interfaces for machine learning models."
      ],
      "metadata": {
        "id": "BKKZbGcgf8dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Gradio interface using updated API\n",
        "iface = gr.Interface(\n",
        "    fn=generate_tts,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=10, label=\"Enter Text\"),\n",
        "        gr.Radio(choices=list(CFG.VOICES.keys()), label=\"Select Voice Speaker\")\n",
        "    ],\n",
        "    outputs=gr.Audio(type=\"filepath\", label=\"Generated Audio\"),\n",
        "    title=\"AI-Based Text-to-Speech Tool\",\n",
        "    description=\"Enter text and select a voice to generate speech.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "F6N-dk1kN7TK",
        "outputId": "8a3b9b24-bfb4-4e30-9366-a2afe9c28522"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://fbf759d8f00387f660.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fbf759d8f00387f660.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of sentences in this text: 1\n",
            "\n",
            "Processing sentence 1/1...\n",
            "\n",
            "              Sentence 1/1 processed:\n",
            "              \tNumber of tokens in sentence: 29\n",
            "              \tLength of sentence: 108\n",
            "              \tShape of tensor for this sentence: torch.Size([1, 246080])\n",
            "              \tElapsed time for this sentence: 37.52 s\n",
            "              \n",
            "Final audio saved as final_audio.wav\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://fbf759d8f00387f660.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "总结\n",
        "通过更新  接口组件并确保您的 TTS 模型支持中文（简体）语音生成，您可以创建一个功能强大且用户友好的文本转语音工具。上述示例代码提供了一个全面的框架，您可以根据需要进行调整和优化。如果在实施过程中遇到任何问题，请随时提供详细信息，我将乐意进一步协助您！"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "WFnQjT77N7WX",
        "outputId": "27834d68-9951-4202-fdfe-89f4f8088a1a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '（' (U+FF08) (<ipython-input-33-8d1d8c2d714d>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-8d1d8c2d714d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    通过更新  接口组件并确保您的 TTS 模型支持中文（简体）语音生成，您可以创建一个功能强大且用户友好的文本转语音工具。上述示例代码提供了一个全面的框架，您可以根据需要进行调整和优化。如果在实施过程中遇到任何问题，请随时提供详细信息，我将乐意进一步协助您！\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '（' (U+FF08)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gs0Np9paN7ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLTDcPtiN7cg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}